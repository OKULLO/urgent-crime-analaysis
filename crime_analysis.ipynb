{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gilo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/gilo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/gilo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/gilo/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#Machine Learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics # for confusion matrix, accuracy score etc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started dataset loading: \n",
      "Datasets ready to use!\n",
      "Total Length of DataFrame is 692\n",
      "\n",
      " Study the cat\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetText</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>buglary</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>homicide</th>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rape</th>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>robbery</th>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          TweetText\n",
       "cat                \n",
       "buglary           8\n",
       "homicide        278\n",
       "rape             74\n",
       "robbery         332"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df =data_preprocessing()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                              TweetText       cat\n",
       "2    #Vancouver police homicide squad investigates ...  homicide\n",
       "4    #RCMP dive team in waters off Neck Point Park ...  homicide\n",
       "9    #Homicide. Over a small amount of marijuana an...  homicide\n",
       "13   #MN man who did time for pipe #bombs now wante...  homicide\n",
       "15   âð¼ðâ¦  President of Mongols bikie cha...  homicide\n",
       "..                                                 ...       ...\n",
       "225  Follow our\\n TRAIN OF CHANGE\\nSign our 3 urgen...      rape\n",
       "226  #Petition updates.\\nFigures still slowing down...      rape\n",
       "227  The situation in the #DRC & #SouthSudan calls ...      rape\n",
       "228  The governmentâs #rape review: an apology, b...      rape\n",
       "230  Letâs all stick to the facts.\\nStuart Lubboc...      rape\n",
       "\n",
       "[692 rows x 2 columns]>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example before cleaning at position 4\n",
      "âð¼ðâ¦  President of Mongols bikie chapter arrested over alleged murder of Finks bikie Shane Bowden #Crime #Homicide \n",
      "@abcnews\n",
      " VIA \n",
      "@headlines24_7\n",
      "example after cleaning at position 4\n",
      "President Mongols bikie chapter arrested alleged murder Finks bikie Shane Bowden Crime Homicide VIA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:27: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n"
     ]
    }
   ],
   "source": [
    "cleaner =clean_tweets(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One Dead Double Shooting Citgo Gas Station Wyoming Ave MI Headlines via detroit wyomingave citgo homicide'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(cleaner.TweetText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  0.9981916817359855 \n",
      " The validation accuracy is:  0.9784172661870504\n",
      "confusion matrix\n",
      "[[ 3  0  0  0]\n",
      " [ 0 51  0  2]\n",
      " [ 0  0 16  0]\n",
      " [ 0  1  0 66]]\n",
      "check classes on which we trained\n",
      "['buglary' 'homicide' 'rape' 'robbery']\n",
      "[8.88808665e-07 1.17353457e-05 0.00000000e+00 ... 0.00000000e+00\n",
      " 9.01295884e-04 1.15845111e-04]\n",
      "Feature ranking:\n",
      "['homicide', 'robbery', 'rape', 'death', 'may', 'truecrime', 'de', 'buglary', 'sexualassault', 'murder', 'montreal', 'violence', 'investigation', 'rapeculture', 'delhi', 'atm', 'india', 'metoo', 'mumbai', 'pour', 'robbed', 'minor', 'another', 'armed', 'replying', 'shooting', 'sirrapedr', 'gib', 'read', 'girl', 'femicide', 'unsolved', 'gohabsgo', 'un', 'hospital', 'homme', 'sbi', 'killed', 'sexual', 'women', 'le', 'crime', 'others', 'roh', 'tvgujaraticom', 'today', 'cctv', 'jun', 'fatal', 'rajasthan']\n"
     ]
    }
   ],
   "source": [
    "forest, vectorizer, feature_names, importances = classify_forest(cleaner.TweetText,cleaner.cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.64390285e-07 0.00000000e+00 1.61510539e-04 ... 0.00000000e+00\n",
      " 9.37577203e-04 1.31666269e-04]\n"
     ]
    }
   ],
   "source": [
    "print(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cList = {\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"I'd\": \"I would\",\n",
    "  \"I'd've\": \"I would have\",\n",
    "  \"I'll\": \"I will\",\n",
    "  \"I'll've\": \"I will have\",\n",
    "  \"i'm\": \"i am\",\n",
    "  \"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it had\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that's\": \"that is\",\n",
    "  \"there'd\": \"there had\",\n",
    "  \"there'd've\": \"there would have\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"to've\": \"to have\",\n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"wont\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"y'alls\": \"you alls\",\n",
    "  \"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\n",
    "  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",\n",
    "  \"you'd\": \"you had\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you will\",\n",
    "  \"you'll've\": \"you will have\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"youre\": \"you are\",\n",
    "  \"you've\": \"you have\",\n",
    "  \"youve\": \"you have\",\n",
    "  \"yall\": \"you all\",\n",
    "  \"ya'll\": \"you all\"\n",
    "}\n",
    "\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    text = c_re.sub(replace, text.lower())\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "tweets = list(df.TweetText)\n",
    "\n",
    "for i in tweets:\n",
    "    \n",
    "    df['text'] = expandContractions(i)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " train,test = train_test_split(cleaner.TweetText, random_state=0, test_size=.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "553"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing():\n",
    "    \n",
    "\n",
    "    print('Started dataset loading: ')\n",
    "    \n",
    "    \n",
    "#     fl =\"metoo_tweets_dec2017.csv\"\n",
    "    homicide = pd.read_csv('homicide_data.csv',skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    homicide['cat'] = 'homicide'\n",
    "\n",
    "    rape = pd.read_csv('twitter_data.csv',skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    rape['cat'] = 'rape'\n",
    "\n",
    "    robbery = pd.read_csv('robbery_data.csv',skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    robbery['cat'] = 'robbery'\n",
    "\n",
    "    buglary = pd.read_csv('barglary_data.csv',skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    buglary['cat'] = 'buglary'\n",
    "\n",
    "    \n",
    "    #make into one dataframe\n",
    "    \n",
    "    df = pd.concat([homicide,robbery,buglary,rape])\n",
    "\n",
    "\n",
    "    #drop unneccessary columns\n",
    "    df = df.drop(['User','Handle',\n",
    "                  'ReplyCount','RetweetCount','PostDate'], axis=1)\n",
    "    \n",
    "#     #drop nan values\n",
    "    df = df.dropna()\n",
    "    df = df[['TweetText','cat']]\n",
    "    \n",
    "    print('Datasets ready to use!')\n",
    "    \n",
    "    print('Total Length of DataFrame is', len(df))\n",
    "    \n",
    "    print('\\n Study the cat')\n",
    "    display(df.groupby(['cat']).count())\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  clean_tweets(df,hashtag_just_sign=True, remove_stopwords=True,test_position=4,remove_digits=True):\n",
    "    print('example before cleaning at position', test_position)\n",
    "    print(df.iloc[test_position].TweetText)\n",
    "    \n",
    "    #IN ANY CASE \n",
    "    #non alpabetical/ numerical\n",
    "    df = df.replace(to_replace =r'&amp;', value = '', regex = True)\n",
    "    df = df.replace(to_replace =r'&gt;', value = '', regex = True)\n",
    "    #hyperlinks\n",
    "    df = df.replace(to_replace =r'http\\S+', value = '', regex = True)\n",
    "    #usernames\n",
    "    df = df.replace(to_replace =r'@\\S+', value = '', regex = True) \n",
    "    #remove retweet\n",
    "    df = df.replace(to_replace ='RT :', value = '', regex = True) \n",
    "    df = df.replace(to_replace ='RT ', value = '', regex = True)\n",
    "    \n",
    "    #HASHTAG OPTIONAL REMOVE\n",
    "    if(hashtag_just_sign == True):\n",
    "        df = df.replace(to_replace ='#', value = '', regex = True) \n",
    "    else:\n",
    "        df = df.replace(to_replace =r'#\\S+', value = '', regex = True)\n",
    "        \n",
    "    #IN ANY CASE\n",
    "    #remove punctation\n",
    "    df = df.replace(to_replace ='[\",:!?\\\\-]', value = ' ', regex = True)\n",
    "    #4. Tokenize into words (all lower case)\n",
    "    df.text = df.TweetText.str.lower()\n",
    "    \n",
    "    #make sure no weird letters left\n",
    "    #remove digits\n",
    "    df = df.replace(to_replace ='[\\d+]', value = '', regex = True)\n",
    "    #special characters\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    \n",
    "    df = df.replace(to_replace =pattern, value = '', regex = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #u = df.select_dtypes(object)\n",
    "    df['TweetText'] = df['TweetText'].apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\n",
    "    \n",
    "    if(remove_stopwords == True):\n",
    "        df.TweetText = df.TweetText.str.split() \n",
    "        eng_stopwords = set(stopwords.words(\"english\"))\n",
    "        df['TweetText'] = df['TweetText'].apply(lambda x: [item for item in x if item not in eng_stopwords])\n",
    "        #join the list items back to one string\n",
    "        df['TweetText'] = df['TweetText'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    \n",
    "    \n",
    "    print('example after cleaning at position', test_position)\n",
    "    print(df.iloc[test_position].TweetText)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classify datasets\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_forest(data, y):\n",
    "\n",
    "    print(\"Creating the bag of words model!\\n\")\n",
    "    # CountVectorizer\" is scikit-learn's bag of words tool, here we show more keywords \n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 max_features = 2000) \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "    data, y, random_state=0, test_size=.2)\n",
    "\n",
    "    # Then we use fit_transform() to fit the model / learn the vocabulary,\n",
    "    # then transform the data into feature vectors.\n",
    "    # The input should be a list of strings. .toarraty() converts to a numpy array\n",
    "    \n",
    "    train_bag = vectorizer.fit_transform(X_train).toarray()\n",
    "    test_bag = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "    # You can extract the vocabulary created by CountVectorizer\n",
    "    # by running print(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "    print(\"Training the random forest classifier!\\n\")\n",
    "    # Initialize a Random Forest classifier with 75 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 50) \n",
    "\n",
    "    # Fit the forest to the training set, using the bag of words as \n",
    "    # features and the sentiment labels as the target variable\n",
    "    forest = forest.fit(train_bag, y_train)\n",
    "\n",
    "    train_predictions = forest.predict(train_bag)\n",
    "    test_predictions = forest.predict(test_bag)\n",
    "    \n",
    "    train_acc = metrics.accuracy_score(y_train, train_predictions)\n",
    "    valid_acc = metrics.accuracy_score(y_test, test_predictions)\n",
    "    print(\"The training accuracy is: \", train_acc, \"\\n\", \"The validation accuracy is: \", valid_acc)\n",
    "    \n",
    "    print('confusion matrix')\n",
    "    print(metrics.confusion_matrix(y_test,test_predictions))\n",
    "    \n",
    "    print('check classes on which we trained')\n",
    "    print(forest.classes_)\n",
    "    \n",
    "    importances = forest.feature_importances_\n",
    "    # returns relative importance of all features.\n",
    "    # they are in the order of the columns\n",
    "    print(importances)\n",
    "    len(importances)\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    # sort importance scores\n",
    "    \n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "    top_50 = indices[:50]\n",
    "    top50_features = [vectorizer.get_feature_names()[ind] for ind in top_50]\n",
    "    print(top50_features)\n",
    "    \n",
    "    return(forest, vectorizer, feature_names, importances)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robbery\n"
     ]
    }
   ],
   "source": [
    "cls =forest.predict(vectorizer.transform(['help am being chased, he has a panga']).astype('float'))[0]\n",
    "print(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:29<00:00,  7.42it/s]\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import tqdm\n",
    "from nltk.corpus import words\n",
    "\n",
    "joblib.dump(forest, '../urgent/service_NLP/models/ml_model.jb')\n",
    "joblib.dump(vectorizer, '../urgent/service_NLP/models/vectorizer.jb')\n",
    "\n",
    "dtf = pd.DataFrame(np.vstack((feature_names, importances))).transpose()\n",
    "dtf.columns = ['feature', 'importance']\n",
    "dtf.set_index('feature', inplace=True)\n",
    "msk = np.asarray([np.any([c.isdigit() for c in w]) for w in dtf.index])\n",
    "dtf = dtf[~msk]\n",
    "exi = np.asarray([w in set(words.words()) for w in tqdm.tqdm(dtf.index)])\n",
    "dtf[exi].to_parquet('../urgent/service_NLP/models/vocabulary.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
